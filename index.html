<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>CL ≈ NSCL v2</title>
    <link rel="icon" type="image/png" href="lab-logo.png" />

    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>

    <!-- KaTeX CSS + JS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" />
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"></script>

    <!-- Icons (Font Awesome) -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" />

    <style>
      .fade-in-up {
        opacity: 0;
        transform: translateY(20px);
        transition: opacity 0.6s ease-out, transform 0.6s ease-out;
      }
      .fade-in-up.visible {
        opacity: 1;
        transform: translateY(0);
      }
      body {
        scroll-behavior: smooth;
      }
    </style>
  </head>
  <body class="bg-slate-50 text-slate-800">
    <main class="flex flex-col items-center px-4 py-10 min-h-screen">
      <!-- Hero Section -->
      <section id="hero" class="w-full max-w-5xl text-center fade-in-up">
        <h1 class="mb-4 text-4xl font-extrabold leading-tight md:text-5xl">
          On the Alignment Between Supervised and <br class="hidden md:block" />
          Self-Supervised Contrastive Learning
        </h1>
        <div class="mt-2 space-y-1 text-lg font-medium text-slate-700 md:text-2xl">
          <p>
            <a href="https://achleshwar.github.io/" class="hover:underline" target="_blank">Achleshwar Luthra</a>
            · <a href="https://www.linkedin.com/in/priyadarsi-mishra/" class="hover:underline" target="_blank">Priyadarsi Mishra</a>
            · <a href="https://tomergalanti.github.io/index.html" class="hover:underline" target="_blank">Tomer Galanti</a>
          </p>
          <p class="text-slate-500 font-medium">Texas A&amp; University</p>
          <p class="text-slate-600 italic">Paper under review</p>
        </div>
      </section>

      <!-- Links Section -->
      <section class="mt-10 flex flex-col md:flex-row items-center justify-center gap-8">
        <a href="https://github.com/DLFundamentals/understanding_ssl_v2" class="flex flex-col items-center hover:scale-105 transition-transform" target="_blank">
          <i class="fab fa-github text-5xl text-slate-800"></i>
          <span class="mt-2 text-md text-slate-600">Code</span>
        </a>
        <a href="#" class="flex flex-col items-center hover:scale-105 transition-transform" target="_blank">
          <i class="fas fa-file-pdf text-5xl text-indigo-500"></i>
          <span class="mt-2 text-md text-slate-600">Paper</span>
        </a>
      </section>

      <!-- Overview Section -->
      <section class="mt-20 w-full max-w-5xl text-left">
        <h2 class="text-2xl font-bold mb-4">Overview</h2>
        <p class="text-lg mb-4">
          Self-supervised contrastive learning (CL) has achieved remarkable success, producing representations that rival supervised pre-training. 
          <a href="https://dlfundamentals.github.io/ssl-is-approximately-sl/" class="text-blue-600 hover:underline">Recent work</a> has shown that the CL loss closely approximates a supervised surrogate, 
          <strong>Negatives-only Supervised Contrastive Learning (NSCL)</strong>, as the number of classes grows. 
          But this similarity at the loss-level leaves a crucial question unresolved.
        </p>

        <div class="fade-in-up mx-auto my-8 w-full max-w-3xl rounded-md border border-black bg-indigo-50 px-6 py-4 text-center text-lg italic text-slate-800 shadow-sm">
          <strong>
            Do <span class="text-red-600">contrastive</span> and <span class="text-blue-600">supervised</span> models remain aligned throughout training, not just at the level of their objectives?
          </strong>
        </div>

        <p class="text-lg">
          Understanding this dynamic relationship is crucial. A close alignment throughout training would suggest that CL's learning process inherently mimics a supervised signal, providing a stronger 
          foundation for its empirical success and the transferability of its learned features. In this study, we analyze the alignment between CL and NSCL and find <strong class="text-red-600">a notable divergence in 
          their parameter-space trajectories</strong>. Despite this, we demonstrate both that their learned <strong class="text-green-600">representations remain remarkably aligned</strong>.
          We provide theoretical guarantees for this representation alignment and a better understanding of how this alignment varies with more classes, higher temperatures and batch-size.
        </p>
      </section>

      <!-- Methodology Section -->
      <section class="mt-20 w-full max-w-5xl text-left">
        <h2 class="text-2xl font-bold mb-4">Methodology</h2>
        <p class="text-lg mb-4">
            To investigate this, we conduct a controlled study focusing on the self-supervised contrastive loss, $L^{CL}$, and its supervised counterpart, $L^{NSCL}$. 
            We train two models under identical conditions—shared initialization, mini-batches, augmentations, and hyperparameters—to isolate the effect of the loss function itself.
        </p>
        <!-- Loss Equations -->
        <div class="flex flex-col md:flex-row gap-4 mb-4">
              <!-- DCL Equation -->
              <div class="border-2 border-red-600 bg-slate-50 rounded p-4 text-sm flex-1 hover:border-red-300">
                <h4 class="text-center font-semibold mb-2 text-red-700">Self-supervised Contrastive Loss (CL)</h4>
                $$\small
                \mathcal{L}^{\mathrm{CL}}(f) = -\frac{1}{K^2N}\sum^{K}_{l_1,l_2=1}\sum_{i=1}^N
                \log \left(
                \frac{\exp(\mathrm{sim}(z^{l_1}_i, z^{l_2}_i)/\tau)}{
                \sum^{K}_{l_3=1} \textcolor{red}{\sum_{j\in [N]\setminus \{i\}}} \exp (\mathrm{sim}(z^{l_1}_i, z^{l_3}_j)/\tau) }
                \right)
                $$
              </div>

              <!-- NSCL Equation -->
              <div class="border-2 border-blue-600 bg-slate-50 rounded p-4 text-sm flex-1 hover:border-blue-300">
                <h4 class="text-center font-semibold mb-2 text-blue-700">Negatives-only Supervised Contrastive Loss (NSCL)</h4>
                $$\small
                \mathcal{L}^{\mathrm{NSCL}}(f) = -\frac{1}{K^2N}\sum^{K}_{l_1,l_2=1}\sum_{i=1}^N
                \log \left(
                \frac{\exp(\mathrm{sim}(z^{l_1}_i, z^{l_2}_i)/\tau)}{
                \sum^{K}_{l_3=1} \textcolor{blue}{\sum_{j: y_j \neq y_i}} \exp (\mathrm{sim}(z^{l_1}_i, z^{l_3}_j)/\tau) }
                \right)
                $$
              </div>
            </div>


            <ul class="list-disc list-inside text-sm text-slate-600 space-y-1">
            <li><strong>$\mathrm{sim(\cdot, \cdot)}$</strong> denotes cosine similarity</li>
            <li><strong>$N$</strong>: Total number of <strong>training</strong> samples</li>
            <li><strong>$K$</strong>: Total number of augmented versions of each sample</li>
            <li><strong>$z^l_i = f(\alpha_k(x_i))$</strong>, where <strong>$x_i$</strong> is an input image
              and <strong>$\alpha_k$</strong> is its $k^{th}$ augmentation.</li>
            </ul>
        </div>
      </section>
      
      <!-- Key Claims -->
      <section class="mt-20 w-full max-w-5xl text-left">
        <h2 class="text-2xl font-bold mb-6">Key Observations</h2>

        <!-- Claim 1 -->
        <div class="mb-10 p-6 border rounded-md shadow-sm bg-white fade-in-up">
            <h3 class="text-xl font-semibold mb-4">1 · Divergence in Weights vs. Alignment in Representations</h3>
            <p class="mb-4">
                Our central finding is a tale of two spaces. When trained with shared randomness (initialization, batches, augmentations), 
                <span class="text-red-600 font-medium">CL</span> and <span class="text-blue-700 font-medium">NSCL</span> models take different paths in parameter space, 
                leading to a significant weight gap. However, the representational geometry they induce remains remarkably similar.
            </p>
            
            <div class="grid grid-cols-1 md:grid-cols-2 gap-6 items-center">
              <div class="flex flex-col items-center">
                <img src="images/weight_space_angle.png" alt="Weight Space Divergence" class="w-full object-contain rounded shadow transition hover:scale-105" />
                <span class="mt-2 text-sm text-slate-500 text-center"><strong>(a) Weight Space:</strong> Model parameters diverge significantly as training progresses.</span>
              </div>
              <div class="flex flex-col items-center">
                <img src="images/rep_space_angle.png" alt="Representation Space Alignment" class="w-full object-contain rounded shadow transition hover:scale-105" />
                <span class="mt-2 text-sm text-slate-500 text-center"><strong>(b) Representation Space:</strong> Learned features remain closely aligned, maintaining high similarity.</span>
              </div>
            </div>
             <p class="text-sm text-slate-500 text-center max-w-4xl mx-auto italic mt-4">
              Corresponds to Figure 1 in the paper. See Appendix A for additional details.
            </p>
        </div>
        <!-- End Claim 1 -->

        <p class="text-lg font-medium text-red-400 mb-4">TODOs</p>
        <ul class="list-disc list-inside text-red-400 mb-6">
          <li>Explain evaluation metrics and datasets.</li>
          <li>Ensure all mathematical expressions are correctly formatted and displayed.</li>
          <li>Verify all links (Code, Paper) point to the correct destinations.</li>
          <li>Double-check all citations and references for accuracy.</li>
        </ul>

        <p class="text-lg mb-4">
            The divergence in weight space suggests that directly comparing model parameters isn't the full story. What truly matters for downstream performance 
            is the <em>geometry</em> of the learned representations. To better quantify this, we analyze the alignment in "similarity space." Instead of tracking millions 
            of parameters, we track the $N \times N$ pairwise similarity matrix, $\Sigma$, whose entries $\Sigma_{ij}$ represent the cosine similarity between the embeddings 
            of two inputs, $x_i$ and $x_j$. This perspective allows us to directly measure how the geometric structure of the representation space evolves and prove the 
            following bound.
        </p>
        <div class="p-4 bg-indigo-50 border border-indigo-200 rounded-md">
           <p class="mb-2 text-center text-sm md:text-base">
              $$||\Sigma_{T}^{\textcolor{red}{\text{CL}}}-\Sigma_{T}^{\textcolor{blue}{\text{NSCL}}}||_{F} 
              \le 
              \exp\left(\frac{1}{2\tau^{2}B}\sum_{t=0}^{T-1}\eta_{t}\right)\frac{1}{\tau\sqrt{B}}\left(\sum_{t=0}^{T-1}\eta_{t}\right) \cdot \mathcal{O}\left(\frac{e^{2/\tau}}{C}\right)$$
            </p>
            <p class="text-sm italic text-right text-slate-500 mb-4">Theorem (1)</p>
        </div>
        
        <!-- Note on CKA and RSA -->
        <p class="text-md mt-6">
            <strong>Note on Metrics:</strong> Theorem (1) provides a direct bound on the difference between the two similarity matrices. 
            In our experiments, we use standard and widely-accepted metrics—<a href="https://arxiv.org/abs/1905.00414" class="text-blue-600 hover:underline">Centered Kernel Alignment (CKA)</a> 
            and <a href="https://www.frontiersin.org/journals/systems-neuroscience/articles/10.3389/neuro.06.004.2008/full" class="text-blue-600 hover:underline">Representational Similarity Analysis (RSA)</a>
            —to empirically measure alignment.
            These metrics are fundamentally dependent on the underlying similarity matrices; a small distance between the matrices directly implies high CKA and RSA scores. 
            For the formal derivation connecting our bound to these specific metrics, we refer the interested reader to Corollaries 1 and 2 in our paper.
        </p>

        <!-- Claim 2 - Factors Influencing Alignment -->
        <section class="mt-10 mb-10 w-full fade-in-up">
            <h3 class="text-xl font-semibold mb-2">2 · Factors Influencing Alignment</h3>
            <p class="mb-6 text-slate-600">
                Our theory predicts how alignment should behave under different conditions. We empirically confirm these predictions across multiple datasets by varying key hyperparameters.
            </p>
        
            <!-- Tab Buttons -->
            <div class="mb-4 border-b border-slate-200">
                <nav class="-mb-px flex space-x-6" aria-label="Tabs">
                    <button onclick="showTab('classes')" id="tab-classes" class="tab-button whitespace-nowrap py-4 px-1 border-b-2 font-medium text-sm border-indigo-500 text-indigo-600">
                        <i class="fas fa-list-ol mr-2"></i> Number of Classes (C)
                    </button>
                    <button onclick="showTab('temp')" id="tab-temp" class="tab-button whitespace-nowrap py-4 px-1 border-b-2 font-medium text-sm border-transparent text-slate-500 hover:text-slate-700 hover:border-slate-300">
                        <i class="fas fa-thermometer-half mr-2"></i> Temperature (τ)
                    </button>
                    <button onclick="showTab('batch')" id="tab-batch" class="tab-button whitespace-nowrap py-4 px-1 border-b-2 font-medium text-sm border-transparent text-slate-500 hover:text-slate-700 hover:border-slate-300">
                        <i class="fas fa-box mr-2"></i> Batch Size (B)
                    </button>
                </nav>
            </div>
        
            <!-- Tab Content -->
            <div class="bg-white p-6 border rounded-md shadow-sm">
                <!-- Content for Number of Classes -->
                <div id="content-classes" class="tab-content">
                    <p class="mb-4 text-slate-700"><strong>Finding:</strong> Alignment gets stronger as the number of classes increases. This holds consistently across different datasets.</p>
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-6">
                        <div class="text-center space-y-2">
                            <img src="images/classes/mini_imagenet_cka_heatmap_train-1.png" alt="Mini-ImageNet Classes Experiment - Train" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                            <img src="images/classes/mini_imagenet_cka_heatmap_test-1.png" alt="Mini-ImageNet Classes Experiment - Test" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                            <span class="mt-2 text-sm text-slate-500 block pt-2">Mini-ImageNet</span>
                        </div>
                        <div class="text-center space-y-2">
                            <img src="images/classes/tiny_imagenet_cka_heatmap_train-1.png" alt="Tiny-ImageNet Classes Experiment - Train" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                            <img src="images/classes/tiny_imagenet_cka_heatmap_test-1.png" alt="Tiny-ImageNet Classes Experiment - Test" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                            <span class="mt-2 text-sm text-slate-500 block pt-2">Tiny-ImageNet</span>
                        </div>
                         <div class="text-center space-y-2">
                            <img src="images/classes/full_imagenet_cka_heatmap_train-1.png" alt="ImageNet-1k Classes Experiment - Train" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                             <img src="images/classes/full_imagenet_cka_heatmap_test-1.png" alt="ImageNet-1k Classes Experiment - Test" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                            <span class="mt-2 text-sm text-slate-500 block pt-2">ImageNet-1k</span>
                        </div>
                    </div>
                    <p class="text-sm text-slate-500 text-center max-w-4xl mx-auto italic mt-4">Corresponds to Figure 3 in the paper. The heatmaps show the linear CKA between CL and NSCL models on both train (top, green) and test (bottom, purple) datasets.</p>
                </div>
        
                <!-- Content for Temperature -->
                <div id="content-temp" class="tab-content hidden">
                    <p class="mb-4 text-slate-700"><strong>Finding:</strong> Higher temperatures lead to better and more stable alignment. A temperature of τ=1.0 consistently outperforms lower values.</p>
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-6">
                        <div class="text-center space-y-2">
                            <img src="images/tau/cifar100_rsa_plot-1.png" alt="CIFAR-100 Temperature Experiment - RSA" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                            <img src="images/tau/cifar100_cka_plot-1.png" alt="CIFAR-100 Temperature Experiment - CKA" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                            <span class="mt-2 text-sm text-slate-500 block pt-2">CIFAR-100</span>
                        </div>
                        <div class="text-center space-y-2">
                           <img src="images/tau/tiny_imagenet_rsa_plot-1.png" alt="Tiny-ImageNet Temperature Experiment - RSA" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                            <img src="images/tau/tiny_imagenet_cka_plot-1.png" alt="Tiny-ImageNet Temperature Experiment - CKA" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                            <span class="mt-2 text-sm text-slate-500 block pt-2">Tiny-ImageNet</span>
                        </div>
                         <div class="text-center space-y-2">
                            <img src="images/tau/mini_imagenet_rsa_plot-1.png" alt="Mini-ImageNet Temperature Experiment - RSA" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                            <img src="images/tau/mini_imagenet_cka_plot-1.png" alt="Mini-ImageNet Temperature Experiment - CKA" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                            <span class="mt-2 text-sm text-slate-500 block pt-2">Mini-ImageNet</span>
                        </div>
                    </div>
                    <p class="text-sm text-slate-500 text-center max-w-4xl mx-auto italic mt-4">Corresponds to Figure 4 in the paper. The plots show RSA (top) and CKA (bottom)
                      over 300 epochs between CL and NSCL models trained with different temperatures.</p>
                    </p>
                </div>
        
                <!-- Content for Batch Size -->
                <div id="content-batch" class="tab-content hidden">
                    <p class="mb-4 text-slate-700"><strong>Finding:</strong> The effect of batch size is coupled with the learning rate schedule. With appropriate scaling, larger batch sizes generally improve alignment.</p>
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-6">
                        <div class="text-center space-y-2">
                            <img src="images/batches/lr_linear/mini_imagenet_rsa_plot-1.png" alt="Linear Batch Size Experiment - RSA" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                            <img src="images/batches/lr_linear/mini_imagenet_cka_plot-1.png" alt="Linear Batch Size Experiment - CKA" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                            <span class="mt-2 text-sm text-slate-500 block pt-2">$$\mathcal{O}(B)$$</span>
                        </div>
                        <div class="text-center space-y-2">
                           <img src="images/batches/lr_sqrt/mini_imagenet_rsa_plot-1.png" alt="Sqrt Batch Size Experiment - RSA" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                            <img src="images/batches/lr_sqrt/mini_imagenet_cka_plot-1.png" alt="Sqrt Batch Size Experiment - CKA" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                            <span class="mt-2 text-sm text-slate-500 block pt-2">$$\mathcal{O}(B^{1/2})$$</span>
                        </div>
                         <div class="text-center space-y-2">
                           <img src="images/batches/lr_sqrt_4/mini_imagenet_rsa_plot-1.png" alt="Sqrt_4 Batch Size Experiment - RSA" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                            <img src="images/batches/lr_sqrt_4/mini_imagenet_cka_plot-1.png" alt="Sqrt_4 Batch Size Experiment - CKA" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                            <span class="mt-2 text-sm text-slate-500 block pt-2">$$\mathcal{O}(B^{1/4})$$</span>
                        </div>
                    </div>
                     <p class="text-sm text-slate-500 text-center max-w-4xl mx-auto italic mt-4">Corresponds to Figure 5 in the paper. Here we vary $\eta$ with $B$ across three cases: $\eta=\tfrac{0.3B}{256}$, 
                      $\eta=\tfrac{0.3\sqrt{B}}{256}$, and $\eta=\tfrac{0.3\sqrt[4]{B}}{256}$. Under $\mathcal{O}(B)$ scaling, CL–NSCL alignment decreases as $B$ grows whereas for the other two cases, alignment increases with $B$. </p>
                </div>
            </div>
        </section>

        <!-- Segue Text -->
        <p class="text-lg my-8 fade-in-up">
            So far, we've demonstrated that the alignment between CL and NSCL is not just theoretical but holds up empirically, influenced by factors like class count, temperature, and batch size. But this raises a natural question: why focus specifically on NSCL as the supervised benchmark? Is it truly the best proxy for understanding self-supervised CL, compared to other supervised methods?
        </p>

        <!-- key question 2--> 
        <div class="fade-in-up mx-auto my-8 w-full max-w-3xl rounded-md border border-black bg-indigo-50 px-6 py-4 text-center text-lg italic text-slate-800 shadow-sm">
          <strong>
             How does the <span class="text-red-600 font-medium">CL</span>-<span class="text-blue-700 font-medium">NSCL</span> alignment compare to other supervised objectives? 
          </strong>
        </div>
        <!-- Claim 3 -->
        <div class="mt-10 mb-10 p-6 border rounded-md shadow-sm bg-white fade-in-up">
            <h3 class="text-xl font-semibold mb-2">3 · NSCL is the Best Supervised Proxy</h3>
            <p class="mb-4">
                We find that NSCL is a much better proxy for CL than both standard Supervised Contrastive Learning (SCL) and Cross-Entropy (CE). This positions NSCL as a principled bridge for understanding self-supervised learning.
            </p>
            <div class="grid grid-cols-1 md:grid-cols-3 gap-6">
                <div class="text-center space-y-2">
                    <img src="images/epochs/cifar100_rsa_plot-1.png" alt="CIFAR-100 Temperature Experiment - RSA" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                    <img src="images/epochs/cifar100_cka_plot-1.png" alt="CIFAR-100 Temperature Experiment - CKA" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                    <span class="mt-2 text-sm text-slate-500 block pt-2">CIFAR-100</span>
                </div>
                <div class="text-center space-y-2">
                    <img src="images/epochs/tiny_imagenet_rsa_plot-1.png" alt="Tiny-ImageNet Temperature Experiment - RSA" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                    <img src="images/epochs/tiny_imagenet_cka_plot-1.png" alt="Tiny-ImageNet Temperature Experiment - CKA" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                    <span class="mt-2 text-sm text-slate-500 block pt-2">Tiny-ImageNet</span>
                </div>
                  <div class="text-center space-y-2">
                    <img src="images/epochs/mini_imagenet_rsa_plot-1.png" alt="Mini-ImageNet Temperature Experiment - RSA" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                    <img src="images/epochs/mini_imagenet_cka_plot-1.png" alt="Mini-ImageNet Temperature Experiment - CKA" class="w-full object-contain rounded shadow-md transition hover:scale-105" />
                    <span class="mt-2 text-sm text-slate-500 block pt-2">Mini-ImageNet</span>
                </div>
            </div>
             <p class="text-sm text-slate-500 text-center max-w-4xl mx-auto italic mt-4">
              Corresponds to Figure 2 in the paper. We train CL, NSCL, SCL and CE models with ResNet-50 on CIFAR-100, Tiny-ImageNet and Mini-ImageNet datasets.
              The plots show RSA (top) and CKA (bottom) between CL and the three supervised objectives. NSCL consistently achieves the highest alignment with CL.
            </p>
        </div>
      </section>

      <!-- Final Remarks Section -->
      <section class="mt-20 w-full max-w-5xl text-left fade-in-up">
        <h2 class="text-2xl font-bold mb-4">Final Remarks</h2>
        <p class="text-lg leading-relaxed text-slate-700 mb-6">
            Our results highlight that the implicit supervised signal in CL is not confined to its loss function but extends throughout the entire optimization trajectory. By showing that CL and NSCL representations co-evolve in a stable and coupled manner, we provide a stronger theoretical bridge between supervised and self-supervised learning.
        </p>
        <div class="flex flex-col md:flex-row gap-4">
            <a href="#" target="_blank"
            class="px-6 py-3 text-white bg-indigo-600 hover:bg-indigo-700 rounded text-center font-medium transition">
            Read the Paper
            </a>
            <a href="#" target="_blank"
            class="px-6 py-3 border border-indigo-600 text-indigo-600 hover:bg-indigo-50 rounded text-center font-medium transition">
            View Code on GitHub
            </a>
        </div>
      </section>

      <!-- Bibtex Section -->
      <section id="bibtex" class="mt-24 max-w-5xl w-full fade-in-up">
        <h2 class="text-2xl font-bold mb-4">BibTeX</h2>
        <div class="relative">
      <pre id="bibtex-entry" class="bg-slate-900 text-green-200 p-4 rounded overflow-x-auto text-sm leading-relaxed font-mono">
      @misc{clnscl2025alignment,
          title={On the Alignment Between Supervised and Self-Supervised Contrastive Learning}, 
          author={Achleshwar Luthra and Priyadarsi Mishra and Tomer Galanti},
          year={2025},
          publisher={arXiv},
      }
      </pre>
            <button onclick="copyBibtex(event)" class="absolute top-2 right-2 px-3 py-1 text-xs bg-slate-800 text-white border border-slate-700 rounded hover:bg-slate-700 transition">
                <i class="fas fa-copy mr-1"></i> Copy
            </button>
        </div>
      </section>

      <!-- Footer -->
      <footer class="mt-20 border-t pt-6 text-center text-sm text-slate-500">
        © 2025 Built with ☕. <br />
        Contact: <a href="mailto:luthra@tamu.edu" class="underline">luthra@tamu.edu</a>
        &middot; <a href="mailto:galanti@tamu.edu" class="underline">galanti@tamu.edu</a>
      </footer>
    </main>

    <!-- JS for animations and copy button -->
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ]
        });

        const observer = new IntersectionObserver(
          (entries) => {
            entries.forEach((entry) => {
              if (entry.isIntersecting) {
                entry.target.classList.add('visible');
              }
            });
          },
          { threshold: 0.1 }
        );
        document.querySelectorAll('.fade-in-up').forEach((el) => observer.observe(el));
      });

      function copyBibtex(event) {
        const text = document.getElementById("bibtex-entry").innerText;
        const textArea = document.createElement("textarea");
        textArea.value = text;
        document.body.appendChild(textArea);
        textArea.select();
        document.execCommand('copy');
        document.body.removeChild(textArea);
        
        const button = event.currentTarget;
        button.innerHTML = '<i class="fas fa-check mr-1"></i> Copied!';
        setTimeout(() => {
            button.innerHTML = '<i class="fas fa-copy mr-1"></i> Copy';
        }, 2000);
      }

      function showTab(tabName) {
        document.querySelectorAll('.tab-content').forEach(content => {
            content.classList.add('hidden');
        });

        document.querySelectorAll('.tab-button').forEach(button => {
            button.classList.remove('border-indigo-500', 'text-indigo-600');
            button.classList.add('border-transparent', 'text-slate-500', 'hover:text-slate-700', 'hover:border-slate-300');
        });

        document.getElementById('content-' + tabName).classList.remove('hidden');

        const activeButton = document.getElementById('tab-' + tabName);
        activeButton.classList.add('border-indigo-500', 'text-indigo-600');
        activeButton.classList.remove('border-transparent', 'text-slate-500', 'hover:text-slate-700', 'hover:border-slate-300');
    }
    </script>
  </body>
</html>

