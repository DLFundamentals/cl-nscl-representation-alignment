<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>CL & NSCL Alignment</title>
    <link rel="icon" type="image/png" href="lab-logo.png" />

    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>

    <!-- KaTeX CSS + JS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" />
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"></script>

    <!-- Icons (Font Awesome) -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" />

    <style>
      .fade-in-up {
        opacity: 0;
        transform: translateY(20px);
        transition: opacity 0.6s ease-out, transform 0.6s ease-out;
      }
      .fade-in-up.visible {
        opacity: 1;
        transform: translateY(0);
      }
      body {
        scroll-behavior: smooth;
      }
    </style>
  </head>
  <body class="bg-slate-50 text-slate-800">
    <main class="flex flex-col items-center px-4 py-10 min-h-screen">
      <!-- Hero Section -->
      <section id="hero" class="w-full max-w-5xl text-center fade-in-up">
        <h1 class="mb-4 text-4xl font-extrabold leading-tight md:text-5xl">
          On the Alignment Between Supervised and <br class="hidden md:block" />
          Self-Supervised Contrastive Learning
        </h1>
        <div class="mt-2 space-y-1 text-lg font-medium text-slate-700 md:text-2xl">
          <p>
            <a href="https://achleshwar.github.io/" class="hover:underline" target="_blank">Achleshwar Luthra</a>
            · <a href="https://www.linkedin.com/in/priyadarsi-mishra/" class="hover:underline" target="_blank">Priyadarsi Mishra</a>
            · <a href="https://tomergalanti.github.io/index.html" class="hover:underline" target="_blank">Tomer Galanti</a>
          </p>
          <p class="text-slate-500 font-medium">Texas A&amp;M University</p>
          <p class="text-slate-600 italic">Under review</p>
        </div>
      </section>

      <!-- Links Section -->
      <section class="mt-10 flex flex-col md:flex-row items-center justify-center gap-8">
        <a href="#" class="flex flex-col items-center hover:scale-105 transition-transform" target="_blank">
          <i class="fab fa-github text-5xl text-slate-800"></i>
          <span class="mt-2 text-md text-slate-600">Code</span>
        </a>
        <a href="#" class="flex flex-col items-center hover:scale-105 transition-transform" target="_blank">
          <i class="fas fa-file-pdf text-5xl text-indigo-500"></i>
          <span class="mt-2 text-md text-slate-600">Paper</span>
        </a>
      </section>

      <!-- Overview Section -->
      <section class="mt-20 w-full max-w-5xl text-left">
        <h2 class="text-2xl font-bold mb-4">Overview</h2>
        <p class="text-lg mb-4">
          Self-supervised contrastive learning (CL) has achieved remarkable success, producing representations that rival supervised pre-training. 
          Recent theory has shown that the CL loss closely approximates a supervised surrogate, Negatives-Only Supervised Contrastive Learning (NSCL), as the number of classes grows. 
          But this similarity at the loss-level leaves a crucial question unresolved.
        </p>

        <div class="fade-in-up mx-auto my-8 w-full max-w-3xl rounded-md border border-black bg-indigo-50 px-6 py-4 text-center text-lg italic text-slate-800 shadow-sm">
          <strong>
            Do contrastive and supervised models remain aligned throughout training, not just at the level of their objectives?
          </strong>
        </div>

        <p class="text-lg">
          In this work, we show that while their parameters <strong class="text-red-600">diverge in weight space</strong>, their learned features <strong class="text-blue-700">remain aligned in representation space</strong>.
          We provide theoretical guarantees for this representation alignment and empirically validate that this alignment strengthens with more classes and higher temperatures.
        </p>
      </section>
      
      <!-- Key Claims -->
      <section class="mt-20 w-full max-w-5xl text-left">
        <h2 class="text-2xl font-bold mb-6">Key Observations</h2>

        <!-- Claim 1 -->
        <div class="mb-10 p-6 border rounded-md shadow-sm bg-white fade-in-up">
            <h3 class="text-xl font-semibold mb-4">1 · Divergence in Weights vs. Alignment in Representations</h3>
            <p class="mb-4">
                Our central finding is a tale of two spaces. When trained with shared randomness (initialization, batches, augmentations), <span class="text-red-600 font-medium">CL</span> and <span class="text-blue-700 font-medium">NSCL</span> models take different paths in parameter space, leading to a significant weight gap. However, the representational geometry they induce remains remarkably similar, as measured by metrics like CKA and RSA.
            </p>
            
            <div class="grid grid-cols-1 md:grid-cols-2 gap-6 items-center">
              <div class="flex flex-col items-center">
                <img src="https://placehold.co/600x400/fecaca/b91c1c?text=Weight+Space+Divergence" alt="Weight Space Divergence" class="w-full object-contain rounded shadow transition hover:scale-105" />
                <span class="mt-2 text-sm text-slate-500 text-center"><strong>(a) Weight Space:</strong> Model parameters diverge significantly as training progresses.</span>
              </div>
              <div class="flex flex-col items-center">
                <img src="https://placehold.co/600x400/dbeafe/1d4ed8?text=Representation+Space+Alignment" alt="Representation Space Alignment" class="w-full object-contain rounded shadow transition hover:scale-105" />
                <span class="mt-2 text-sm text-slate-500 text-center"><strong>(b) Representation Space:</strong> Learned features remain closely aligned, maintaining high similarity.</span>
              </div>
            </div>
             <p class="text-sm text-slate-500 text-center max-w-4xl mx-auto italic mt-4">
              Corresponds to Figures 1, 6 in the paper.
            </p>

            <p class="mt-6">
              We prove that the Frobenius norm distance between the similarity matrices is bounded, ensuring this alignment.
            </p>
            <p class="mb-2 text-center text-sm md:text-base">
                $$||\Sigma_{T}^{\textcolor{red}{\text{CL}}}-\Sigma_{T}^{\textcolor{blue}{\text{NSCL}}}||_{F} \le \exp\left(\frac{1}{2\tau^{2}B}\sum_{t=0}^{T-1}\eta_{t}\right)\frac{1}{\tau\sqrt{B}}\left(\sum_{t=0}^{T-1}\eta_{t}\right)\Delta_{C,\delta}(B;\tau)$$
            </p>
            <p class="text-sm italic text-right text-slate-500 mb-4">Theorem (1)</p>
        </div>

        <!-- Claim 2 -->
        <div class="mt-10 mb-10 p-6 border rounded-md shadow-sm bg-white fade-in-up">
            <h3 class="text-xl font-semibold mb-2">2 · Factors Influencing Alignment</h3>
            <p class="mb-4">
                Our theory predicts how alignment should behave under different conditions. We empirically confirm these predictions:
            </p>
            <ul class="list-disc list-inside text-base text-slate-800 space-y-2 mb-6">
                <li><strong>Number of Classes (C):</strong> Alignment gets stronger as the number of classes increases.</li>
                <li><strong>Temperature ($\tau$):</strong> Higher temperatures lead to better and more stable alignment.</li>
                <li><strong>Batch Size (B):</strong> The effect of batch size is coupled with the learning rate schedule, but generally, larger batches improve alignment.</li>
            </ul>
             <div class="grid grid-cols-1 md:grid-cols-2 gap-6 items-center">
              <div class="flex flex-col items-center">
                <img src="https://placehold.co/600x400/e0e7ff/4338ca?text=Alignment+vs+Classes" alt="Alignment vs Number of Classes" class="w-full object-contain rounded shadow transition hover:scale-105" />
                <span class="mt-2 text-sm text-slate-500 text-center">More classes lead to higher CKA/RSA.</span>
              </div>
              <div class="flex flex-col items-center">
                <img src="https://placehold.co/600x400/fef3c7/b45309?text=Alignment+vs+Temperature" alt="Alignment vs Temperature" class="w-full object-contain rounded shadow transition hover:scale-105" />
                <span class="mt-2 text-sm text-slate-500 text-center">Higher temperature ($\tau=1.0$) improves alignment.</span>
              </div>
            </div>
            <p class="text-sm text-slate-500 text-center max-w-4xl mx-auto italic mt-4">
              Corresponds to Figures 3, 4, 5 in the paper.
            </p>
        </div>

        <!-- Claim 3 -->
        <div class="mt-10 mb-10 p-6 border rounded-md shadow-sm bg-white fade-in-up">
            <h3 class="text-xl font-semibold mb-2">3 · NSCL is the Best Supervised Proxy</h3>
            <p class="mb-4">
                How does the <span class="text-red-600 font-medium">CL</span>-<span class="text-blue-700 font-medium">NSCL</span> alignment compare to other supervised objectives? We find that NSCL is a much better proxy for CL than both standard Supervised Contrastive Learning (SCL) and Cross-Entropy (CE). This positions NSCL as a principled bridge for understanding self-supervised learning.
            </p>
            <div class="flex flex-col items-center">
               <img src="https://placehold.co/800x400/e5e7eb/1f2937?text=Alignment+Comparison" alt="Alignment of CL with NSCL, SCL, and CE" class="w-full max-w-lg object-contain rounded shadow transition hover:scale-105" />
               <p class="text-sm text-slate-500 text-center max-w-4xl mx-auto italic mt-4">
                  Figure 2: CL-NSCL alignment (blue) is consistently higher than CL-SCL (red) and CL-CE (green).
               </p>
            </div>
        </div>
      </section>

      <!-- Final Remarks Section -->
      <section class="mt-20 w-full max-w-5xl text-left fade-in-up">
        <h2 class="text-2xl font-bold mb-4">Final Remarks</h2>
        <p class="text-lg leading-relaxed text-slate-700 mb-6">
            Our results highlight that the implicit supervised signal in CL is not confined to its loss function but extends throughout the entire optimization trajectory. By showing that CL and NSCL representations co-evolve in a stable and coupled manner, we provide a stronger theoretical bridge between supervised and self-supervised learning.
        </p>
        <div class="flex flex-col md:flex-row gap-4">
            <a href="#" target="_blank"
            class="px-6 py-3 text-white bg-indigo-600 hover:bg-indigo-700 rounded text-center font-medium transition">
            Read the Paper
            </a>
            <a href="#" target="_blank"
            class="px-6 py-3 border border-indigo-600 text-indigo-600 hover:bg-indigo-50 rounded text-center font-medium transition">
            View Code on GitHub
            </a>
        </div>
      </section>

      <!-- Bibtex Section -->
      <section id="bibtex" class="mt-24 max-w-5xl w-full fade-in-up">
        <h2 class="text-2xl font-bold mb-4">BibTeX</h2>
        <div class="relative">
<pre id="bibtex-entry" class="bg-slate-900 text-green-200 p-4 rounded overflow-x-auto text-sm leading-relaxed font-mono">
@misc{clnscl2026alignment,
    title={On the Alignment Between Supervised and Self-Supervised Contrastive Learning}, 
    author={Achleshwar Luthra and Priyadarsi Mishra and Tomer Galanti},
    year={2026},
    publisher={arxiv}
}
</pre>
            <button onclick="copyBibtex(event)" class="absolute top-2 right-2 px-3 py-1 text-xs bg-slate-800 text-white border border-slate-700 rounded hover:bg-slate-700 transition">
                <i class="fas fa-copy mr-1"></i> Copy
            </button>
        </div>
      </section>

      <!-- Footer -->
      <footer class="mt-20 border-t pt-6 text-center text-sm text-slate-500 w-full max-w-5xl">
        © 2025 | Website template adapted from <a href="https://github.com/achleshwar/achleshwar.github.io" class="underline" target="_blank">here</a>.
      </footer>
    </main>

    <!-- JS for animations and copy button -->
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        // KaTeX is loaded with defer, so it should be available here.
        // This function will run after the DOM is ready, fixing the render error.
        renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ]
        });

        // Intersection Observer for animations
        const observer = new IntersectionObserver(
          (entries) => {
            entries.forEach((entry) => {
              if (entry.isIntersecting) {
                entry.target.classList.add('visible');
              }
            });
          },
          { threshold: 0.1 }
        );
        document.querySelectorAll('.fade-in-up').forEach((el) => observer.observe(el));
      });

      function copyBibtex(event) {
        const text = document.getElementById("bibtex-entry").innerText;
        const textArea = document.createElement("textarea");
        textArea.value = text;
        document.body.appendChild(textArea);
        textArea.select();
        document.execCommand('copy');
        document.body.removeChild(textArea);
        
        // Use event.currentTarget to ensure we get the button
        const button = event.currentTarget;
        button.innerHTML = '<i class="fas fa-check mr-1"></i> Copied!';
        setTimeout(() => {
            button.innerHTML = '<i class="fas fa-copy mr-1"></i> Copy';
        }, 2000);
      }
    </script>
  </body>
</html>

